{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f8d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/6k/lvj277mx6834z6rvr_nxth_00000gn/T/jieba.cache\n",
      "Loading model cost 0.568 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import gensim\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import string\n",
    "\n",
    "# Ignore DeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('GOLDEN_merged.csv', encoding=\"utf-8-sig\")\n",
    "\n",
    "# Load user-defined dictionary\n",
    "jieba.load_userdict(\"../dict/dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for cleaning and tokenization\n",
    "def clean(sentences):\n",
    "    cleaned = []\n",
    "    for sent in sentences:\n",
    "        # Remove unwanted characters and patterns\n",
    "        sent = re.sub(r\"^\\d{1,2}-\\d{1,2}-\\d{1,2}\\s\\d{1,2}:\\d{1,2}\\(\\d+\\.\\d+\\s(?:KB|MB)\\)\\s*\", '', sent)\n",
    "        sent = sent.lower()\n",
    "#         sent = re.sub(\"[~^0-9]\", \"\", sent)\n",
    "        sent = re.sub(r'http\\S+|https\\S+', '', sent)\n",
    "        sent = BeautifulSoup(sent, 'html.parser').get_text(separator=' ')\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation + '，。！？；：‘’“”（）《》【】、'))\n",
    "        sent = sent.replace('emoji', '').replace('已pm', '').replace('pm', '').replace('thx', '').replace('pls', '').replace('kb', '').replace('mb', '').replace('向左走', '').replace('向右走', '').replace('死死', '')\n",
    "        sent = re.sub('\\s+', ' ', sent).strip()\n",
    "        cleaned.append(sent)\n",
    "    return cleaned\n",
    "\n",
    "df['clean'] = clean(df['merged'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(sentence):\n",
    "    # Segment Chinese text and remove stopwords\n",
    "    stopwords = [line.strip() for line in open('../dict/canton.txt', 'r', encoding='utf-8').readlines()]\n",
    "    outstr = ''\n",
    "    for word in jieba.cut(sentence.strip()):\n",
    "        if word not in stopwords and (len(word.strip()) > 0) and (word >= '\\u4e00' and word <= '\\u9fa5'):\n",
    "            outstr += word + \" \"\n",
    "    return outstr\n",
    "\n",
    "df['tokenz'] = [segment(sent) for sent in df['clean']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process text data\n",
    "def process(texts):\n",
    "    # Build bigram and trigram models and apply to tokenized text\n",
    "    bigram = gensim.models.Phrases(texts, min_count=10, threshold=10)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], min_count=10, threshold=10)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Load stopwords and Cantonese-specific words\n",
    "    stop_words = [line.strip() for line in open('../dict/stopwords.txt', 'r', encoding='UTF-8').readlines()]\n",
    "    canton = [line.strip() for line in open('../dict/canton.txt', 'r', encoding='UTF-8').readlines()]\n",
    "\n",
    "    # Define a function to remove stopwords, parts of speech, and Cantonese-specific words\n",
    "    def remove_words(doc):\n",
    "        return [word for word in gensim.utils.simple_preprocess(str(doc), min_len=2, deacc=True)\n",
    "                if word not in stop_words and word not in canton]\n",
    "\n",
    "    # Apply processing steps to tokenized text\n",
    "    texts = [[word for word in trigram_mod[bigram_mod[remove_words(doc)]]] for doc in texts]\n",
    "    texts = [[word for sent in doc for word in sent.split() if not re.match(r'[^\\w\\s]', word)] for doc in texts]\n",
    "    return texts\n",
    "\n",
    "# Apply the processing function to the 'tokenz' column of a DataFrame\n",
    "df['tokenz'] = process(df['tokenz'])\n",
    "\n",
    "# Define a function to split sentences into individual words and remove punctuation\n",
    "def sent_to_words_space(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub(r'[^\\w\\s]', '', sent)  # remove punctuations\n",
    "        yield sent\n",
    "\n",
    "# Convert the 'tokenz' column to a list of strings\n",
    "df['tokenz'] = df['tokenz'].astype(str)\n",
    "\n",
    "# Convert the 'tokenz' column into a list of lists of individual words\n",
    "msg_space = df.tokenz.values.tolist()\n",
    "msg_space_words = list(sent_to_words_space(msg_space))\n",
    "\n",
    "# Assign the list of lists of individual words back to the 'tokenz' column\n",
    "df['tokenz'] = msg_space_words\n",
    "\n",
    "# Print the final DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"token.csv\", encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
